{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "import tensorflow as tf\n",
    "import h5py\n",
    "import os\n",
    "from tensorflow.keras import layers, models, optimizers, callbacks, backend\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_svhn_data(val_size):\n",
    "    with h5py.File('SVHN_train.hdf5', 'r') as f:\n",
    "        shape = f[\"X\"].shape\n",
    "        x_train = f[\"X\"][:shape[0]-val_size]\n",
    "        y_train = f[\"Y\"][:shape[0]-val_size].flatten()\n",
    "        x_val = f[\"X\"][shape[0]-val_size:]\n",
    "        y_val = f[\"Y\"][shape[0] - val_size:].flatten()\n",
    "\n",
    "    with h5py.File('SVHN_test.hdf5', 'r') as f:\n",
    "        x_test = f[\"X\"][:]\n",
    "        y_test = f[\"Y\"][:].flatten()\n",
    "\n",
    "    y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "    y_val = tf.keras.utils.to_categorical(y_val, 10)\n",
    "    y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "    return (x_train, y_train), (x_val, y_val), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(32, 32, 3)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    \n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    \n",
    "    model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    \n",
    "#     model.add(layers.Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "#     model.add(layers.BatchNormalization())\n",
    "#     model.add(layers.Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "#     model.add(layers.MaxPooling2D((2, 2)))\n",
    "#     model.add(layers.Dropout(0.3))\n",
    "    \n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(512, activation='relu'))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "#     lr = 0.0001\n",
    "#     opt = {\"SGD\": optimizers.SGD(lr=lr), \"Adam\": optimizers.Adam(lr=lr)}\n",
    "    \n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    train_data, val_data, test_data = load_svhn_data(val_size=10000)\n",
    "\n",
    "    model = build_model()\n",
    "    \n",
    "    save_model = callbacks.ModelCheckpoint(\"weights.hdf5\",\n",
    "                                           monitor='val_accuracy',\n",
    "                                           verbose=1,\n",
    "                                           save_best_only=True,\n",
    "                                           save_weights_only=False)\n",
    "    \n",
    "    early_stopping = callbacks.EarlyStopping(monitor='val_accuracy',\n",
    "                                             min_delta=0,\n",
    "                                             patience=3,\n",
    "                                             verbose=0,\n",
    "                                             mode='max')\n",
    "    tensorboard = callbacks.TensorBoard(histogram_freq=10,\n",
    "                                        batch_size=32,\n",
    "                                        write_graph=True,\n",
    "                                        write_grads=False, write_images=False, embeddings_freq=0,\n",
    "                                        embeddings_layer_names=None, embeddings_metadata=None)\n",
    "    \n",
    "    print(\"TRAINING MODEL\")\n",
    "    history = model.fit(train_data[0], train_data[1],\n",
    "                        batch_size=100,\n",
    "                        epochs=50,\n",
    "                        validation_data=val_data,\n",
    "                        callbacks=[early_stopping, save_model])\n",
    "\n",
    "    print(\"DONE TRAINING MODEL\")\n",
    "    print(\"EVALUATING MODEL\")\n",
    "    model = models.load_model(\"weights.hdf5\")\n",
    "    score = model.evaluate(test_data[0], test_data[1], verbose=0)\n",
    "    print('Test loss: {:.4f}'.format(score[0]))\n",
    "    print('Test accuracy: {:.4f}'.format(score[1]))\n",
    "    print(\"DONE\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, img_path, batch_size):\n",
    "    model = models.load_model(model)\n",
    "    # normalize image pixel values into range [0,1]\n",
    "    img_generator = image.ImageDataGenerator(preprocessing_function=lambda img: img/255.0)\n",
    "    validation_generator = img_generator.flow_from_directory(directory=img_path, target_size=(32,32), shuffle=False,\n",
    "                                                             batch_size=batch_size, color_mode=\"rgb\")\n",
    "\n",
    "    score = model.evaluate_generator(validation_generator)\n",
    "    print(\"Accuracy: {:.4f}\".format(score[1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_8 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 4, 4, 256)         295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 4, 4, 256)         1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 920,266\n",
      "Trainable params: 919,306\n",
      "Non-trainable params: 960\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:`batch_size` is no longer needed in the `TensorBoard` Callback and will be ignored in TensorFlow 2.0.\n",
      "TRAINING MODEL\n",
      "Train on 63257 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "63200/63257 [============================>.] - ETA: 0s - loss: 1.4433 - accuracy: 0.5112\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.80340, saving model to weights.hdf5\n",
      "63257/63257 [==============================] - 223s 4ms/sample - loss: 1.4426 - accuracy: 0.5113 - val_loss: 0.6061 - val_accuracy: 0.8034\n",
      "Epoch 2/50\n",
      "63200/63257 [============================>.] - ETA: 0s - loss: 0.5925 - accuracy: 0.8132\n",
      "Epoch 00002: val_accuracy improved from 0.80340 to 0.86460, saving model to weights.hdf5\n",
      "63257/63257 [==============================] - 212s 3ms/sample - loss: 0.5923 - accuracy: 0.8133 - val_loss: 0.4544 - val_accuracy: 0.8646\n",
      "Epoch 3/50\n",
      "63200/63257 [============================>.] - ETA: 0s - loss: 0.4906 - accuracy: 0.8488\n",
      "Epoch 00003: val_accuracy improved from 0.86460 to 0.89970, saving model to weights.hdf5\n",
      "63257/63257 [==============================] - 212s 3ms/sample - loss: 0.4906 - accuracy: 0.8488 - val_loss: 0.3372 - val_accuracy: 0.8997\n",
      "Epoch 4/50\n",
      "63200/63257 [============================>.] - ETA: 0s - loss: 0.4284 - accuracy: 0.8670\n",
      "Epoch 00004: val_accuracy improved from 0.89970 to 0.90700, saving model to weights.hdf5\n",
      "63257/63257 [==============================] - 211s 3ms/sample - loss: 0.4283 - accuracy: 0.8671 - val_loss: 0.3113 - val_accuracy: 0.9070\n",
      "Epoch 5/50\n",
      "63200/63257 [============================>.] - ETA: 0s - loss: 0.3924 - accuracy: 0.8793\n",
      "Epoch 00005: val_accuracy improved from 0.90700 to 0.91110, saving model to weights.hdf5\n",
      "63257/63257 [==============================] - 212s 3ms/sample - loss: 0.3923 - accuracy: 0.8793 - val_loss: 0.3065 - val_accuracy: 0.9111\n",
      "Epoch 6/50\n",
      "63200/63257 [============================>.] - ETA: 0s - loss: 0.3678 - accuracy: 0.8882\n",
      "Epoch 00006: val_accuracy did not improve from 0.91110\n",
      "63257/63257 [==============================] - 211s 3ms/sample - loss: 0.3679 - accuracy: 0.8882 - val_loss: 0.3311 - val_accuracy: 0.9028\n",
      "Epoch 7/50\n",
      "63200/63257 [============================>.] - ETA: 0s - loss: 0.3474 - accuracy: 0.8936\n",
      "Epoch 00007: val_accuracy improved from 0.91110 to 0.92630, saving model to weights.hdf5\n",
      "63257/63257 [==============================] - 211s 3ms/sample - loss: 0.3473 - accuracy: 0.8937 - val_loss: 0.2735 - val_accuracy: 0.9263\n",
      "Epoch 8/50\n",
      "63200/63257 [============================>.] - ETA: 0s - loss: 0.3374 - accuracy: 0.8976\n",
      "Epoch 00008: val_accuracy did not improve from 0.92630\n",
      "63257/63257 [==============================] - 211s 3ms/sample - loss: 0.3375 - accuracy: 0.8975 - val_loss: 0.2758 - val_accuracy: 0.9254\n",
      "Epoch 9/50\n",
      "63200/63257 [============================>.] - ETA: 0s - loss: 0.3218 - accuracy: 0.9014\n",
      "Epoch 00009: val_accuracy improved from 0.92630 to 0.92940, saving model to weights.hdf5\n",
      "63257/63257 [==============================] - 211s 3ms/sample - loss: 0.3218 - accuracy: 0.9014 - val_loss: 0.2631 - val_accuracy: 0.9294\n",
      "Epoch 10/50\n",
      "63200/63257 [============================>.] - ETA: 0s - loss: 0.3051 - accuracy: 0.9072\n",
      "Epoch 00010: val_accuracy did not improve from 0.92940\n",
      "63257/63257 [==============================] - 211s 3ms/sample - loss: 0.3055 - accuracy: 0.9071 - val_loss: 0.2738 - val_accuracy: 0.9250\n",
      "Epoch 11/50\n",
      "63200/63257 [============================>.] - ETA: 0s - loss: 0.2947 - accuracy: 0.9094\n",
      "Epoch 00011: val_accuracy did not improve from 0.92940\n",
      "63257/63257 [==============================] - 210s 3ms/sample - loss: 0.2951 - accuracy: 0.9094 - val_loss: 0.2706 - val_accuracy: 0.9251\n",
      "Epoch 12/50\n",
      "63200/63257 [============================>.] - ETA: 0s - loss: 0.2815 - accuracy: 0.9135\n",
      "Epoch 00012: val_accuracy did not improve from 0.92940\n",
      "63257/63257 [==============================] - 210s 3ms/sample - loss: 0.2814 - accuracy: 0.9135 - val_loss: 0.2859 - val_accuracy: 0.9243\n",
      "DONE TRAINING MODEL\n",
      "EVALUATING MODEL\n",
      "Test loss: 0.2721\n",
      "Test accuracy: 0.9246\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_images_detection(image_path):\n",
    "    cv2.imread()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv_proj",
   "language": "python",
   "name": "cv_proj"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
